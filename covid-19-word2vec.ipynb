{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nlp/osabo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "import spacy\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import nltk; nltk.download('stopwords')\n",
    "import seaborn as sns\n",
    "import operator\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# sys.path.insert(0, \"../\")\n",
    "\n",
    "root_path = '/home/nlp/data/covid19papers/'\n",
    "\n",
    "\n",
    "corona_features = {\"doc_id\": [None], \"source\": [None], \"title\": [None],\n",
    "                  \"abstract\": [None], \"text_body\": [None]}\n",
    "corona_df = pd.DataFrame.from_dict(corona_features)\n",
    "\n",
    "json_filenames = glob.glob(f'{root_path}/**/*.json', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_corona_df(json_filenames, df, source):\n",
    "\n",
    "    for file_name in json_filenames:\n",
    "\n",
    "        row = {\"doc_id\": None, \"source\": None, \"title\": None,\n",
    "              \"abstract\": None, \"text_body\": None}\n",
    "\n",
    "        with open(file_name) as json_data:\n",
    "            data = json.load(json_data)\n",
    "\n",
    "            doc_id = data['paper_id']\n",
    "            row['doc_id'] = doc_id\n",
    "            row['title'] = data['metadata']['title']\n",
    "\n",
    "            # Now need all of abstract. Put it all in \n",
    "            # a list then use str.join() to split it\n",
    "            # into paragraphs. \n",
    "\n",
    "            abstract_list = [abst['text'] for abst in data['abstract']]\n",
    "            abstract = \"\\n \".join(abstract_list)\n",
    "\n",
    "            row['abstract'] = abstract\n",
    "\n",
    "            # And lastly the body of the text. \n",
    "            body_list = [bt['text'] for bt in data['body_text']]\n",
    "            body = \"\\n \".join(body_list)\n",
    "            \n",
    "            row['text_body'] = body\n",
    "            \n",
    "            # Now just add to the dataframe. \n",
    "            \n",
    "            if source == 'b':\n",
    "                row['source'] = \"BIORXIV\"\n",
    "            elif source == \"c\":\n",
    "                row['source'] = \"COMMON_USE_SUB\"\n",
    "            elif source == \"n\":\n",
    "                row['source'] = \"NON_COMMON_USE\"\n",
    "            elif source == \"p\":\n",
    "                row['source'] = \"PMC_CUSTOM_LICENSE\"\n",
    "            \n",
    "            df = df.append(row, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "corona_df = return_corona_df(json_filenames, corona_df, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.extend(['et', 'al'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentence):\n",
    "    \"\"\"\n",
    "    divides sentence into words and removes punctuations\n",
    "    \"\"\"\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    \"\"\"\n",
    "    removes stopwords\n",
    "    \"\"\"\n",
    "    return [word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words]\n",
    "\n",
    "def get_bigrams(text, bigram_model):\n",
    "    \"\"\"\n",
    "    get bigrams\n",
    "    \"\"\"\n",
    "    return bigram_model[text]\n",
    "\n",
    "def preprocess_text(text, bigram_model):\n",
    "    data_words = list(sent_to_words(text))\n",
    "    data_words = remove_stopwords(data_words)\n",
    "    data_words = get_bigrams(data_words, bigram_model)\n",
    "    return data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = gensim.models.phrases.Phrases(corona_df['text_body'].to_string(), min_count=1, threshold=2)\n",
    "# clean text\n",
    "corona_df['preprocessed_text'] = corona_df['text_body'].apply(preprocess_text, args=(bigram_model,))\n",
    "corona_df.to_csv('corona_preprocessed.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed csv\n",
    "#corona_df = pd.read_csv('corona_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec on Corona Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Word2Vec model\n",
    "model = gensim.models.Word2Vec(corona_df['preprocessed_text'], size=100, window=5, \n",
    "                 min_count=1, workers=4)\n",
    "model.save(\"corona_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(\"corona_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulaize Word Embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "keys = ['coronavirus', 'incubation', 'asymptomatic', 'transmission', 'materials',\n",
    "       'infection', 'diagnostics', 'model', 'morbidities']\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, _ in model.most_similar(word, topn=10):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(model[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "\n",
    "\n",
    "\n",
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "\n",
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=10)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tsne_plot_similar_words('Similar words', keys, embeddings_en_2d, word_clusters, 0.7,\n",
    "                        'similar_words.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA - Topic Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index to word dictionary\n",
    "id2word = gensim.corpora.Dictionary(corona_df['preprocessed_text'])\n",
    "\n",
    "# create indexed text\n",
    "corpus = [id2word.doc2bow(text) for text in corona_df['preprocessed_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_df['lda_label'] =[max(lda_model[c], key=operator.itemgetter(1))[0] for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lda_model, open('lda_model.pk', 'wb'))\n",
    "lda_model = pickle.load(open('lda_model.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(model, topic_range, topn):\n",
    "    word_dict = {};\n",
    "    for i in topic_range:\n",
    "        words = model.show_topic(i, topn);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict);\n",
    "\n",
    "n_words = 10\n",
    "get_topics(lda_model, range(0,n_topics), n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human topic annotation\n",
    "lda_dictionary = {0: 'experiments', 1: 'cells', 2: 'prep_latin', 3 : 'protein', 4: 'model', \n",
    "                  5: 'treatment', 6:'genetics', 7:'infection', 8:'risks', 9:'symptoms'}\n",
    "\n",
    "def transform(num):\n",
    "    return lda_dictionary[num]\n",
    "\n",
    "corona_df['lda_label_string'] = corona_df['lda_label'].apply(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"lda_label_string\",data=corona_df)\n",
    "ax.set_title('Number of text body per topic');\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
